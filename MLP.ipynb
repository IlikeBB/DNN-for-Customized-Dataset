{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/u1/.conda/envs/DLC/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/u1/.conda/envs/DLC/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/u1/.conda/envs/DLC/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/u1/.conda/envs/DLC/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/u1/.conda/envs/DLC/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/u1/.conda/envs/DLC/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import time,sys,string\n",
    "import numba as nb\n",
    "from time import sleep\n",
    "import keras\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# def unpickle(fileName):\n",
    "#     '''\n",
    "#     Description: retrieve data from CIFAR-10 Pickles\n",
    "#     Params: fileName = filename to unpickle\n",
    "#     Outputs: Unpickled Dict\n",
    "#     '''\n",
    "#     with open(fileName, 'rb') as fo:\n",
    "#         dict = pickle.load(fo, encoding='bytes')\n",
    "#     return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_txt(dataset):\n",
    "    label=[]\n",
    "    path=[]\n",
    "    imageid=[]\n",
    "    f=open(r\"%s.txt\"%dataset)\n",
    "    for line in f:\n",
    "        linee=line.split(\" \")\n",
    "        path.append(linee[0])\n",
    "        label.append(int(linee[1]))\n",
    "        MID=linee[0].replace('.JPEG','')\n",
    "        MIDD=MID.split('/')\n",
    "        imageid.append(MIDD[2])\n",
    "    return path,label,imageid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_batches(num_to_load=1):\n",
    "#     '''\n",
    "#     Description: Merge batches of CIFAR-10 data pickles\n",
    "#     Params: num_to_load = number of batches of CIFAR-10 to load and merge\n",
    "#     Outputs: merged features and labels from specified no. of batches of CIFAR-10\n",
    "#     '''\n",
    "#     for i in range(1):\n",
    "#         fileName = \"data_batch_\" + str(i + 1)\n",
    "#         data = unpickle(fileName)\n",
    "#         if i == 0:\n",
    "#             features = data[b\"data\"]\n",
    "#             labels = np.array(data[b\"labels\"])\n",
    "#         else:\n",
    "#             features = np.append(features, data[b\"data\"], axis=0)\n",
    "#             labels = np.append(labels, data[b\"labels\"], axis=0)\n",
    "#     return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def one_hot_encode(data,classes):\n",
    "#     '''\n",
    "#     Description: Encode Target Label IDs to one hot vector of size L where L is the\n",
    "#     number of unique labels\n",
    "#     Params: data = list of label IDs\n",
    "#     Outputs: List of One Hot Vectors\n",
    "#     '''\n",
    "#     one_hot = np.zeros((data.shape[0], classes))\n",
    "#     one_hot[np.arange(data.shape[0]), data] = 1\n",
    "#     return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize(data):\n",
    "#     '''\n",
    "#     Description: Normalize Pixel values\n",
    "#     Params: list of Image Pixel Features\n",
    "#     Outputs: Normalized Image Pixel Features\n",
    "#     '''\n",
    "#     return data / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_info(img_path,img_label,datatype,sizex,sizey,classes):\n",
    "    print('transform  '+datatype)\n",
    "#     temp=0\n",
    "    X_data=[]\n",
    "    label=[]\n",
    "    X_data=np.zeros((len(img_path),sizex,sizey))\n",
    "    label=np.zeros((len(img_path),1))\n",
    "    if(datatype=='test'):\n",
    "        for i in range(len(img_path)):\n",
    "            img=cv2.imread(img_path[i],0)\n",
    "    #         print(img.shape)\n",
    "            img2=cv2.resize(img,(sizex,sizey),interpolation=cv2.INTER_CUBIC)\n",
    "            X_data[i,:,:]=img2\n",
    "            label[i,:]=img_label[i]\n",
    "\n",
    "    #         \n",
    "        X_data/=255.0\n",
    "    #     X_data=X_data.reshape(len(img_path),-1,1)\n",
    "    #     one hot vector\n",
    "        y_label = keras.utils.to_categorical(label, num_classes=classes)\n",
    "        return X_data,y_label,label;\n",
    "    else:\n",
    "        for i in range(len(img_path)):\n",
    "            img=cv2.imread(img_path[i],0)\n",
    "    #         print(img.shape)\n",
    "            img2=cv2.resize(img,(sizex,sizey),interpolation=cv2.INTER_CUBIC)\n",
    "            X_data[i,:,:]=img2\n",
    "            label[i,:]=img_label[i]\n",
    "\n",
    "    #         \n",
    "        X_data/=255.0\n",
    "    #     X_data=X_data.reshape(len(img_path),-1,1)\n",
    "    #     one hot vector\n",
    "        y_label = keras.utils.to_categorical(label, num_classes=classes)\n",
    "        return X_data,y_label;             \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(classes,datatype,sizex,sizey):\n",
    "    '''\n",
    "    Description: helper function to load and preprocess CIFAR-10 training data batches\n",
    "    Params: num_to_load = number of batches of CIFAR-10 to load and merge\n",
    "    Outputs: Pre-processed CIFAR-10 image features and labels\n",
    "    '''\n",
    "    if(datatype=='test'):\n",
    "        path,label,imageid=get_txt(datatype)\n",
    "        X, y ,tl= get_image_info(path,label,datatype,sizex,sizey,classes)\n",
    "    #     X = normalize(X)\n",
    "        print(\"X.shape \"+str(X.shape))\n",
    "        X = X.reshape(-1, sizex*sizey, 1)\n",
    "        print('len(path)  '+str(len(path)))\n",
    "    #     y = one_hot_encode(y,classes)\n",
    "        y = y.reshape(-1, classes, 1)\n",
    "        return X, y, tl\n",
    "    else:\n",
    "        path,label,imageid=get_txt(datatype)\n",
    "        X, y = get_image_info(path,label,datatype,sizex,sizey,classes)\n",
    "    #     X = normalize(X)\n",
    "        print(\"X.shape \"+str(X.shape))\n",
    "        X = X.reshape(-1, sizex*sizey, 1)\n",
    "        print('len(path)  '+str(len(path)))\n",
    "    #     y = one_hot_encode(y,classes)\n",
    "        y = y.reshape(-1, classes, 1)\n",
    "        return X, y        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dataset_split(X, y, ratio=0.8):\n",
    "#     '''\n",
    "#     Description: helper function to split training data into training and validation\n",
    "#     Params: X=image features\n",
    "#             y=labels\n",
    "#             ratio = ratio of training data from total data\n",
    "#     Outputs: training data (features and labels) and validation data\n",
    "#     '''\n",
    "#     split = int(ratio * X.shape[0])\n",
    "#     indices = np.random.permutation(X.shape[0])\n",
    "#     training_idx, val_idx = indices[:split], indices[split:]\n",
    "#     X_train, X_val = X[training_idx, :], X[val_idx, :]\n",
    "#     y_train, y_val = y[training_idx, :], y[val_idx, :]\n",
    "#     print (\"Records in Training Dataset\", X_train.shape[0])\n",
    "#     print (\"Records in Validation Dataset\", X_val.shape[0])\n",
    "#     return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(out):\n",
    "    '''\n",
    "    Description: Sigmoid Activation\n",
    "    Params: out = a list/matrix to perform the activation on\n",
    "    Outputs: Sigmoid activated list/matrix\n",
    "    '''\n",
    "    return 1.0 / (1.0 + np.exp(-out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_sigmoid(out):\n",
    "    '''\n",
    "    Description: Derivative of Sigmoid Activation\n",
    "    Params: out = a list/matrix to perform the activation on\n",
    "    Outputs: Delta(Sigmoid) activated list/matrix\n",
    "    '''\n",
    "    return sigmoid(out) * (1 - sigmoid(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SigmoidCrossEntropyLoss(a, y):\n",
    "        \"\"\"\n",
    "\t\tDescription: Calculate Sigmoid cross entropy loss\n",
    "\t\tParams: a = activation\n",
    "\t\t\t\ty = target one hot vector\n",
    "\t\tOutputs: a loss value\n",
    "        \"\"\"\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(object):\n",
    "    '''\n",
    "            Description: Class to define the Deep Neural Network\n",
    "    '''\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        # setting appropriate dimensions for weights and biases\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        activation = x\n",
    "        activations = [x]  # list to store activations for every layer\n",
    "        outs = []  # list to store out vectors for every layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            out = np.dot(w, activation) + b\n",
    "            outs.append(out)\n",
    "            activation = sigmoid(out)\n",
    "            activations.append(activation)\n",
    "        return outs, activations\n",
    "\n",
    "    def get_batch(self, X, y, batch_size):\n",
    "        for batch_idx in range(0, X.shape[0], batch_size):\n",
    "            batch = zip(X[batch_idx:batch_idx + batch_size],\n",
    "                        y[batch_idx:batch_idx + batch_size])\n",
    "            yield batch\n",
    "\n",
    "    def train(self, X, y, batch_size=100, learning_rate=0.2, epochs=1000):\n",
    "        n_batches = X.shape[0] / batch_size\n",
    "        for j in range(epochs):\n",
    "            batch_iter = self.get_batch(X, y, batch_size)\n",
    "            for i in range(int(n_batches)):\n",
    "                batch = batch_iter.__next__()\n",
    "                # same shape as self.biases\n",
    "                del_b = [np.zeros(b.shape) for b in self.biases]\n",
    "                # same shape as self.weights\n",
    "                del_w = [np.zeros(w.shape) for w in self.weights]\n",
    "                for batch_X, batch_y in batch:\n",
    "                    # accumulate all the bias and weight gradients\n",
    "                    loss, delta_del_b, delta_del_w = self.backpropagate(\n",
    "                        batch_X, batch_y)\n",
    "                    del_b = [db + ddb for db, ddb in zip(del_b, delta_del_b)]\n",
    "                    del_w = [dw + ddw for dw, ddw in zip(del_w, delta_del_w)]\n",
    "            # update weight and biases by multiplying ratio learning rate and batch_size\n",
    "            # multiplied with the accumulated gradients(partial derivatives)\n",
    "            # calculate change in weight(delta) and biases and update weight\n",
    "            # with the changes\n",
    "            self.weights = [w - (learning_rate / batch_size)\n",
    "                            * delw for w, delw in zip(self.weights, del_w)]\n",
    "            self.biases = [b - (learning_rate / batch_size)\n",
    "                           * delb for b, delb in zip(self.biases, del_b)]\n",
    "            print(\"\\nEpoch %d complete\\tLoss: %f\\n\"%(j, loss))\n",
    "\n",
    "    def backpropagate(self, x, y):\n",
    "        del_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        del_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\t\t# for calculating the current loss or cost forward pass through the neural net once\n",
    "\t\t# outs and activations are lists to store activations and out vector\n",
    "\t\t# for every layer\n",
    "        outs, activations = self.feedforward(x)\n",
    "\t\t#Cost function:\n",
    "        loss = SigmoidCrossEntropyLoss(activations[-1],y)\n",
    "\t\t# calculate derivative of cost Sigmoid Cross entropy which is to be minimized\n",
    "        delta_cost = activations[-1] - y\n",
    "\t\t# backward pass to reduce cost\n",
    "\t\t# gradients at output layers\n",
    "        delta = delta_cost\n",
    "        del_b[-1] = delta\n",
    "        del_w[-1] = np.dot(delta, activations[-2].T)\n",
    "\n",
    "\t\t# updating gradients of each layer using reverse or negative indexing, by propagating\n",
    "\t\t# gradients of previous layers to current layer so that gradients of weights and biases\n",
    "\t\t# at each layer can be calculated\n",
    "        for l in range(2, self.num_layers):\n",
    "            out = outs[-l]\n",
    "            delta_activation = delta_sigmoid(out)\n",
    "            delta = np.dot(self.weights[-l + 1].T, delta) * delta_activation\n",
    "            del_b[-l] = delta\n",
    "            del_w[-l] = np.dot(delta, activations[-l - 1].T)\n",
    "        return (loss, del_b, del_w)\n",
    "\n",
    "    def eval(self, X, y):\n",
    "        '''\n",
    "        Description: Based on trained(updated) weights and biases, predict a label and compare\n",
    "                     it with original label and calculate accuracy\n",
    "        Params: X, y = a data example from validation dataset (image features, labels)\n",
    "        Outputs: accuracy of prediction\n",
    "        '''\n",
    "        count = 0\n",
    "        for x, _y in zip(X, y):\n",
    "            outs, activations = self.feedforward(x)\n",
    "            # postion of maximum value is the predicted label\n",
    "            if np.argmax(activations[-1]) == np.argmax(_y):\n",
    "                count += 1\n",
    "        print(\"Accuracy: %f\" % ((float(count) / X.shape[0]) * 100))\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Description: Based on trained(updated) weights and biases, predict a label for an\n",
    "                                 image which does not have a label.\n",
    "        Params: X = list of features of unknown images\n",
    "        Outputs: list containing the predicted label for the corresponding unknown image\n",
    "        '''\n",
    "#         labels = unpickle(\"cifar-10-batches-py/batches.meta\")[\"label_names\"]\n",
    "        preds = np.array([])\n",
    "        for x in X:\n",
    "            outs, activations = self.feedforward(x)\n",
    "            preds = np.append(preds, np.argmax(activations[-1]))\n",
    "#         preds = np.array([labels[int(p)] for p in preds])\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform  train\n",
      "X.shape (63325, 100, 100)\n",
      "len(path)  63325\n",
      "transform  val\n",
      "X.shape (450, 100, 100)\n",
      "len(path)  450\n",
      "transform  test\n",
      "X.shape (450, 100, 100)\n",
      "len(path)  450\n",
      "\n",
      "Epoch 0 complete\tLoss: 70.013510\n",
      "\n",
      "\n",
      "Epoch 1 complete\tLoss: 37.351359\n",
      "\n",
      "\n",
      "Epoch 2 complete\tLoss: 20.047446\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    classes=50\n",
    "    input_x=input_y=100\n",
    "    X_train,y_train = preprocess(classes,'train',input_x,input_y)\n",
    "    X_val,y_val = preprocess(classes,'val',input_x,input_y)\n",
    "    X_test,y_test,y_label = preprocess(classes,'test',input_x,input_y)\n",
    "#     X_train, y_train, X_val, y_val = dataset_split(X, y)\n",
    "#     print(\"X_train.shape \" + str(X_train.shape))\n",
    "#     print(\"y_train.shape \" + str(y_train.shape))\n",
    "#     print(\"X_val.shape \" + str(X_val.shape))\n",
    "#     print(\"y_val.shape \" + str(y_val.shape))    \n",
    "#     print(\"X_test.shape \" + str(X_test.shape))\n",
    "#     print(\"y_test.shape \" + str(y_test.shape))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 32*32*3=3072, height and width of an image in the dataset is 32 and 3 is for RGB channel\n",
    "    #[3072,1000,100,10] implies a neural network with 1 input layer of size 3072, 3 hidden\n",
    "    # layers of size M, N and a output layer of size 10, hence 4\n",
    "    # layers(including input layer), more layers can be added to the list for increasing layers\n",
    "    model = DNN([input_x*input_y, 50, 30, classes])  # initialize the model\n",
    "    model.train(X_train, y_train, epochs=100)  # train the model\n",
    "    model.eval(X_val, y_val)  # check accuracy using validation set\n",
    "    # preprocess test dataset\n",
    "#     test_X = unpickle(\"test_batch\")[b\"data\"] / 255.0\n",
    "#     print(test_X.shape)\n",
    "#     test_X = test_X.reshape(-1, 3072, 1)\n",
    "    # make predictions of test dataset\n",
    "    result=model.predict(X_test)\n",
    "#     print(\"type(result)=\"+str(type(result)))\n",
    "#     print(\"type(y_test)=\"+str(type(y_test)))\n",
    "#     print(\"result[0]=\"+str(result[0]))\n",
    "#     print(\"y_test[0]=\"+str(y_test[0]))\n",
    "       \n",
    "    countt=0\n",
    "    for i in range(len(y_label)):\n",
    "        if (y_label[i]==int(result[i])):\n",
    "            countt=countt+1\n",
    "    print(\"countt=\"+str(countt))\n",
    "    print(\"test_acc=\"+str((countt/len(y_test))))\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
